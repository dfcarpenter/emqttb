:!sectids:
= Documentation

[id=restapi.enabled]
== Enable REST API
`+--restapi+` CLI argument enables REST API (by default it's available at http://127.0.0.0:8017), and it also means that the script keeps running after completing the scenarios.

[id=autorate]
== Autorate configuration

When the loadgen creates too much traffic, the system may get overloaded.
In this case, the test usually has to be restarted all over again with different parameters.
This can be very expensive in man-hours and computing resources.

In order to prevent that, emqttb can tune some parameters (such as message publishing interval)
automatically using https://controlguru.com/integral-reset-windup-jacketing-logic-and-the-velocity-pi-form/[PI controller].

=== Autoscale

A special autorate controlling the rate of spawning new clients is implicitly created for each client group.
Its name follows the pattern `<group_id>_autoscale`.

For example the following command can automatically adjust the rate of connections:

[code,bash]
----
./emqttb @conn -I 10ms -N 10_000 \
   @g --host 172.22.0.13,172.22.0.6 \
   @a -a conn_group_autoscale -V 1000
----

Note: autoscale can be applied to any group used by any scenario.
Additionally, autoscale is used for overload protection, see section about <<value.groups._.scram.threshold,SCRAM>>.

[id=interval]
== Default interval between events

Supported units:

* `us`: microseconds
* `ms`: milliseconds
* `s`: seconds
* `min`: minutes
* `h` : hours

If unit is not specified then `ms` is assumed.

[id=autorate._.id]
== ID of the autorate configuration

Autorate configuration can be referred by id.


[id=scenarios.emqttb_scenario_sub]
== Run scenario sub

This scenario starts `-N` workers, which subscribe to a specified topic.
The only mandatory parameter is `--topic`, which supports pattern substitutions.

=== Client groups

- `sub_group`

[id=scenarios.emqttb_scenario_conn]
== Run scenario conn

This scenario starts `-N` workers, which connect to the broker and then simply linger around.

=== Client groups

- `conn_group`


[id=scenarios.emqttb_scenario_pub]
== Run scenario pub

This scenario starts `-N` workers, which publish messages to the specified topic at period `--pubinterval`.
The only mandatory parameter is `--topic`, which supports pattern substitutions.

=== Examples
==== Basic usage

[code,bash]
----
emqttb @pub -t foo/%n -N 100 -i 10ms -s 1kb
----

In this example the loadgen connects to the default broker <link xlink:href="mqtt://localhost:1883"/>,
starts 100 publishers which send messages to topic with the suffix of the worker id every 10 milliseconds. Size of the messages is 1kb.

==== Changing client settings

[code,bash]
----
emqttb @pub -t foo/%n @g --ssl --transport ws -h 127.0.0.1
----

In this example settings of the default client group has been changed: TLS encryption is enabled, and WebSocket transport is used.
Also the hostname of the broker is specified explicitly.

[code,bash]
----
emqttb @pub -t foo/%n -q 1 -g pub @g -g pub --ssl --transport ws -h 127.0.0.1
----

The below example is similar to the previous one, except QoS of the messages is set to 1,
and a dedicated client configuration with id `pub` is used for the publishers.
It's useful for running multiple scenarios (e.g. `@pub` and `@sub`) in parallel, when they must use
different settings. For example, it can be used for testing MQTT bridge.


==== Tuning publishing rate automatically

By default, `@pub` scenario keeps `pubinterval` constant.
However, in some situations it should be tuned dynamically: suppose one wants to measure what publishing rate the broker can sustain while keeping publish latency under `--publatency`.

This is also useful for preventing system overload.
Generating too much load can bring the system down, and the test has to be started all over again with different parameters.
Sometimes finding the correct rate takes many attempts, wasting human and machine time.
Dynamic tuning of the publishing rate for keeping the latency constant can help in this situation.

By default the maximum speed of rate adjustment is set to 0, effectively locking the `pubinterval` at a constant value.
To enable automatic tuning, the autorate speed `-V` must be set to a non-zero value, also it makes sense to set
the minimum (`-m`) and maximum (`-M`) values of the autorate:

[code,bash]
----
emqttb @pub -t foo -i 1s -q 1 --publatency 50ms @a -V 10 -m 0 -M 10000
----

Once automatic adjustment of the publishing interval is enabled, `-i` parameter sets the starting value of the publish interval,
rather than the constant value. So the above example reads like this:

Publish messages to topic `foo` with QoS 1, starting at the publishing interval of 1000 milliseconds, dynamically adjusting it
so to keep the publishing latency around 50 milliseconds. The publishing interval is kept between 0 and 10 seconds,
and the maximum rate of its change is 10 milliseconds per second.

=== Client groups
- `pub_group`

[id=scenarios.emqttb_scenario_pub._.topic]
== Topic where the clients shall publish messages

Topic is a mandatory parameter. It supports the following substitutions:

* `%n` is replaced with the worker ID (integer)
* `%g` is replaced with the group ID
* `%h` is replaced with the hostname


[id=scenarios.emqttb_scenario_pubsub_forward]
== run scenario pubsub_forward

First all subscribers connect and subscribe to the brokers, then the
publishers start to connect and publish.  The default is to use full
forwarding of messages between the nodes: that is, each publisher
client publishes to a topic subscribed by a single client, and both
clients reside on distinct nodes.

Full forwarding of messages is the default and can be set by full_forwarding.

=== Examples
==== Basic usage

[code,bash]
----
./emqttb --restapi @pubsub_fwd --publatency 10ms --num-clients 400 -i 70ms \
                   @g -h 172.25.0.2:1883,172.25.0.3:1883,172.25.0.4:1883
----

In this example the loadgen connects to a list of brokers
in a round-robin in the declared order.  First all the
subscribers, then the publishers, with the difference that
publishers will shift the given host list by one position
to ensure each publisher and subscriber pair will reside
on different hosts, thus forcing all messages to be
forwarded.

=== Client groups

- `pubsub_fwd_group_pub`
- `pubsub_fwd_group_sub`


[id=scenarios.emqttb_scenario_persistent_session._.pub.qos]
== QoS of the published messages

Warning: changing QoS to any value other then 2 is likely to cause consume stage to hang,
since it has to consume the exact number of messages as previously produced.

[id=scenarios.emqttb_scenario_persistent_session._.sub.qos]
== Subscription QoS

Warning: changing QoS to any value other then 2 is likely to cause consume stage to hang,
since it has to consume the exact number of messages as previously produced.

[id=groups]
== Configuration for client groups
Client configuration is kept separate from the scenario config.
This is done so scenarios could share client configuration.

[id=groups._.net.ifaddr]
== Local IP addresses

Bind a specific local IP address to the connection.
If multiple IP addresses are given, workers choose local address using round-robin algorithm.

WARNING: Setting a local address for a client TCP connection explicitly has a nasty side effect:
when you do this `gen_tpc` calls `bind` on this address to get a free ephemeral port.
But the OS doesn't know that in advance that we won't be listening on the port, so it reserves the local port number for the connection.
However, when we connect to multiple EMQX brokers, we do want to reuse local ports.
So don't use this option when the number of local addresses is less than the number of remote addresses.


[id=groups._.client.clientid]
== Clientid pattern

Pattern used to generate ClientID.
The following substitutions are supported:

* `%n` is replaced with the worker ID (integer)
* `%g` is replaced with the group ID
* `%h` is replaced with the hostname


[id=groups._.connection.keepalive]
== Keepalive time

How often the clients will send `PING` MQTT message to the broker on idle connections.

[id=groups._.scram.threshold]
== Maximum unacked CONNECT packets
`emqttb` can automatically slow down creating new workers in a group when the broker is unable to accept connections in real time.

`emqttb` keeps track of the number of un-acked `CONNECT` packets, and once becomes larger than the threshold,
the group temporarily enters "SCRAM" mode where
it overrides the rate to the <<value.groups.$$_$$.scram.override,specified value>>.
SCRAM mode remains in effect until the number of pending connections becomes less than
_threshold_ * <<value.groups.$$_$$.scram.hysteresis,hystersis>> / 100.

[id=groups._.scram.override]
== SCRAM rate override
Replace whatever configured (or calculated via autorate) connection rate value with this value when broker is not keeping up with the new connections.


[id=groups._.scram.hysteresis]
== SCRAM hysteresis
It's not desirable to switch between normal and SCRAM connection rate too often.


[id=autorate._.update_interval]
== How often autorate is updated

This parameter governs how often error is calculated and control parameter is updated.

[id=autorate._.speed]
== Maximum rate of change of the controlled parameter

Note: this parameter can be set to 0 to effectively disable autorate and lock control parameter in place.
